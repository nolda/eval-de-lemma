{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2896bb4",
   "metadata": {},
   "source": [
    "### Lemmatisierer Evaluierung - qualitativ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc601129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4d86a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baseline', 'germalemma', 'gpt3', 'hanta-pretagged', 'hanta', 'rnntagger', 'simplemma', 'smorlemma', 'spacy2', 'spacy3', 'spacy33+', 'stanza-pretagged', 'stanza', 'trankit', 'treetagger']\n"
     ]
    }
   ],
   "source": [
    "corpora = [l.split('lemmata\\\\')[1].strip('\\\\') for l in glob.glob(\"lemmata/*/\") if not 'overview' in l]\n",
    "algos = [l.split('lemmata/ud-gsd')[1].split('-ud-gsd.csv')[0].strip('\\\\') for l in glob.glob(\"lemmata/ud-gsd/*.csv\")]\n",
    "print(algos)\n",
    "data = []\n",
    "df_all = pd.DataFrame(columns=(['corpus', 'token', 'tag', 'tag_STTS', 'lemma']+algos[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "12f949be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empirist-cmc-blog\n",
      "empirist-cmc-blog-norm\n",
      "empirist-cmc-chat-prof\n",
      "empirist-cmc-chat-prof-norm\n",
      "empirist-cmc-chat-social\n",
      "empirist-cmc-chat-social-norm\n",
      "empirist-cmc-twitter\n",
      "empirist-cmc-twitter-norm\n",
      "empirist-cmc-whatsapp\n",
      "empirist-cmc-whatsapp-norm\n",
      "empirist-cmc-wiki\n",
      "empirist-cmc-wiki-norm\n",
      "empirist-web\n",
      "empirist-web-norm\n",
      "germanc\n",
      "germanc-norm\n",
      "nosta-d-anselm-norm\n",
      "nosta-d-anselm-orig\n",
      "nosta-d-bematac-norm\n",
      "nosta-d-bematac-orig\n",
      "nosta-d-falko-norm\n",
      "nosta-d-falko-orig\n",
      "nosta-d-kafka-norm\n",
      "nosta-d-kafka-orig\n",
      "nosta-d-tuebadz-orig\n",
      "nosta-d-unicum-norm\n",
      "nosta-d-unicum-orig\n",
      "rub2019-novelette\n",
      "rub2019-opensubtitles\n",
      "rub2019-sermononline\n",
      "rub2019-ted\n",
      "rub2019-wikipedia\n",
      "tgermacorp\n",
      "ud-gsd\n",
      "ud-hdt\n",
      "ud-pud\n"
     ]
    }
   ],
   "source": [
    "# gpt3 not lemmatized all sentences\n",
    "gpt3_all = pd.DataFrame(columns=(['corpus', 'token', 'tag', 'tag_STTS', 'lemma', 'gpt3']))\n",
    "                        \n",
    "for c in corpora:\n",
    "    print(c)\n",
    "    gold = pd.read_csv(f\"lemmata/{c}/baseline-{c}.csv\", encoding=\"utf-8\", encoding_errors='ignore')\n",
    "    if os.path.isfile(f\"lemmata/{c}/gpt3-{c}.csv\"):\n",
    "        gpt =  pd.read_csv(f\"lemmata/{c}/gpt3-{c}.csv\", encoding=\"utf-8\", encoding_errors='ignore')\n",
    "        j = 0  # index in gpt3 file\n",
    "        for i, line in gold.iterrows():\n",
    "            if j < len(gpt):\n",
    "                if list(line[:4]) == list(gpt.iloc[j, :4]):\n",
    "                    gpt3_all.loc[len(gpt3_all)] = [c] + list(gpt.iloc[j, :5])\n",
    "                    #print([c] + list(gpt.iloc[j, :4]))\n",
    "                    j += 1\n",
    "                else:\n",
    "                    gpt3_all.append([c] + list(gpt.iloc[j, :4]) + [''])\n",
    "                    j +=1\n",
    "        #joined = gold.join(gpt, how='left', rsuffix='g')\n",
    "    else:\n",
    "        for i, line in gold.iterrows():\n",
    "             gpt3_all.loc[len(gpt3_all)] = [c] +  list(line[:4]) + ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6391d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_all.to_csv(\"lemmata/gpt3_all.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "873ef2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'lemmata/empirist-web/stanza-pretagged-empirist-web.csv' empirist-web not lemmatized by stanza-pretagged\n",
      "\n",
      "[Errno 2] No such file or directory: 'lemmata/empirist-web/stanza-empirist-web.csv' empirist-web not lemmatized by stanza\n",
      "\n",
      "[Errno 2] No such file or directory: 'lemmata/empirist-web-norm/stanza-pretagged-empirist-web-norm.csv' empirist-web-norm not lemmatized by stanza-pretagged\n",
      "\n",
      "[Errno 2] No such file or directory: 'lemmata/empirist-web-norm/stanza-empirist-web-norm.csv' empirist-web-norm not lemmatized by stanza\n",
      "\n",
      "[Errno 2] No such file or directory: 'lemmata/germanc/rnntagger-germanc.csv' germanc not lemmatized by rnntagger\n",
      "\n",
      "[Errno 2] No such file or directory: 'lemmata/ud-hdt/rnntagger-ud-hdt.csv' ud-hdt not lemmatized by rnntagger\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for corpus in corpora:\n",
    "    # prepare dataframe\n",
    "    df = pd.read_csv(f\"lemmata/{corpus}/baseline-{corpus}.csv\", encoding=\"utf-8\")#encoding_errors='ignore'\n",
    "    df['corpus'] = corpus\n",
    "    df.rename(columns = {'lemma_gold': 'lemma'}, inplace = True)\n",
    "    df = df.drop(['lemma_pred'], axis=1)\n",
    "    df = df[['corpus', 'token', 'tag', 'tag_STTS', 'lemma']]\n",
    "    for a in algos[1:]:  # ignore baseline\n",
    "        try:\n",
    "            if not a == 'gpt3':  # not whole corpus lemmatized\n",
    "                df[a] = pd.read_csv(f\"lemmata/{corpus}/{a}-{corpus}.csv\", encoding=\"utf-8\", encoding_errors='ignore')[\"lemma_pred\"]\n",
    "        except Exception as e:  # corpus not lemmatized by algorithm\n",
    "            print(e, f\"{corpus} not lemmatized by {a}\\n\")\n",
    "    data.append(df)\n",
    "    #df.to_csv(f\"lemmata/overall/{corpus}.csv\", encoding=\"utf-8\")\n",
    "    df_all = df_all.append(df)\n",
    "df_all['gpt3'] = gpt3_all['gpt3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1cb55e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude GermanC, corpus not publicly available\n",
    "df_all_wo_germanc = df_all[(df_all.corpus != 'germanc') & (df_all.corpus != 'germanc-norm')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5e6302e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>token</th>\n",
       "      <th>tag</th>\n",
       "      <th>tag_STTS</th>\n",
       "      <th>lemma</th>\n",
       "      <th>germalemma</th>\n",
       "      <th>gpt3</th>\n",
       "      <th>hanta-pretagged</th>\n",
       "      <th>hanta</th>\n",
       "      <th>rnntagger</th>\n",
       "      <th>simplemma</th>\n",
       "      <th>smorlemma</th>\n",
       "      <th>spacy2</th>\n",
       "      <th>spacy3</th>\n",
       "      <th>spacy33+</th>\n",
       "      <th>stanza-pretagged</th>\n",
       "      <th>stanza</th>\n",
       "      <th>trankit</th>\n",
       "      <th>treetagger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empirist-cmc-blog</td>\n",
       "      <td>…</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>$(</td>\n",
       "      <td>…</td>\n",
       "      <td>NaN</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>…</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>empirist-cmc-blog</td>\n",
       "      <td>das</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PDS</td>\n",
       "      <td>der</td>\n",
       "      <td>NaN</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>NaN</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>empirist-cmc-blog</td>\n",
       "      <td>hört</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "      <td>hören</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>empirist-cmc-blog</td>\n",
       "      <td>sich</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRF</td>\n",
       "      <td>sich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sich</td>\n",
       "      <td>sich</td>\n",
       "      <td>sich</td>\n",
       "      <td>sich</td>\n",
       "      <td>er|es|sie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sich</td>\n",
       "      <td>sich</td>\n",
       "      <td>sich</td>\n",
       "      <td>er|es|sie</td>\n",
       "      <td>er|es|sie</td>\n",
       "      <td>er|es|sie</td>\n",
       "      <td>sich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>empirist-cmc-blog</td>\n",
       "      <td>ja</td>\n",
       "      <td>UNK</td>\n",
       "      <td>PTKMA</td>\n",
       "      <td>ja</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21299</th>\n",
       "      <td>ud-pud</td>\n",
       "      <td>Freund</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "      <td></td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "      <td>Freund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21300</th>\n",
       "      <td>ud-pud</td>\n",
       "      <td>des</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>der</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>NaN</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21301</th>\n",
       "      <td>ud-pud</td>\n",
       "      <td>Friedens</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "      <td></td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "      <td>Frieden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21302</th>\n",
       "      <td>ud-pud</td>\n",
       "      <td>erklärte</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBC</td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "      <td></td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "      <td>erklären</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21303</th>\n",
       "      <td>ud-pud</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>--</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1190358 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  corpus     token    tag tag_STTS     lemma germalemma  \\\n",
       "0      empirist-cmc-blog         …  PUNCT       $(         …        NaN   \n",
       "1      empirist-cmc-blog       das   PRON      PDS       der        NaN   \n",
       "2      empirist-cmc-blog      hört   VERB    VVFIN     hören      hören   \n",
       "3      empirist-cmc-blog      sich   PRON      PRF      sich        NaN   \n",
       "4      empirist-cmc-blog        ja    UNK    PTKMA        ja        NaN   \n",
       "...                  ...       ...    ...      ...       ...        ...   \n",
       "21299             ud-pud    Freund   NOUN       NN    Freund     Freund   \n",
       "21300             ud-pud       des    DET       DT       der        NaN   \n",
       "21301             ud-pud  Friedens   NOUN       NN   Frieden    Frieden   \n",
       "21302             ud-pud  erklärte   VERB      VBC  erklären   erklären   \n",
       "21303             ud-pud         .  PUNCT        .         .        NaN   \n",
       "\n",
       "        gpt3 hanta-pretagged     hanta rnntagger  simplemma smorlemma  \\\n",
       "0          …               …         …         …          …         …   \n",
       "1        der             der       der       der        der       NaN   \n",
       "2      hören           hören     hören     hören      hören     hören   \n",
       "3       sich            sich      sich      sich  er|es|sie       NaN   \n",
       "4         ja              ja        ja        ja         ja        ja   \n",
       "...      ...             ...       ...       ...        ...       ...   \n",
       "21299                 Freund    Freund    Freund     Freund    Freund   \n",
       "21300                    der       der       der        der       NaN   \n",
       "21301                Frieden   Frieden   Frieden    Frieden   Frieden   \n",
       "21302               erklären  erklären  erklären   erklären  erklären   \n",
       "21303                      .         .         .          .         .   \n",
       "\n",
       "         spacy2    spacy3  spacy33+ stanza-pretagged     stanza    trankit  \\\n",
       "0             …         …         …              ...        ...          …   \n",
       "1           der       der       der              der        der        der   \n",
       "2         hören     hören     hören            hören      hören      hören   \n",
       "3          sich      sich      sich        er|es|sie  er|es|sie  er|es|sie   \n",
       "4            ja        ja        ja               ja         ja         ja   \n",
       "...         ...       ...       ...              ...        ...        ...   \n",
       "21299    Freund    Freund    Freund           Freund     Freund     Freund   \n",
       "21300       der       der       der              der        der        der   \n",
       "21301   Frieden   Frieden   Frieden          Frieden    Frieden    Frieden   \n",
       "21302  erklären  erklären  erklären         erklären   erklären   erklären   \n",
       "21303         .         .        --                .          .          .   \n",
       "\n",
       "      treetagger  \n",
       "0            ...  \n",
       "1            die  \n",
       "2          hören  \n",
       "3           sich  \n",
       "4             ja  \n",
       "...          ...  \n",
       "21299     Freund  \n",
       "21300        die  \n",
       "21301    Frieden  \n",
       "21302   erklären  \n",
       "21303          .  \n",
       "\n",
       "[1190358 rows x 19 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_wo_germanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "79b19786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_wo_germanc.to_csv(\"lemmata/overview/all.csv\", encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
